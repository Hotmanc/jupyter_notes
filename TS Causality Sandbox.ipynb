{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading Excel and Quick Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\n",
    "\n",
    "3.global_active_power: household global minute-averaged active power (in kilowatt)\n",
    "4.global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n",
    "5.voltage: minute-averaged voltage (in volt)\n",
    "6.global_intensity: household global minute-averaged current intensity (in ampere)\n",
    "7.sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n",
    "8.sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n",
    "9.sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('TS File')\n",
    "df.index = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "df = df.drop(['No', 'year', 'month', 'day', 'hour', 'PM_Jingan', 'PM_US Post', 'PM_Xuhui'], axis = 1)\n",
    "col = df.columns\n",
    "col = col.drop(['season'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Replace values\n",
    "\n",
    "Use interpolate with input time to take care of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.replace(-200, float('nan'))\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "for colnames in col:\n",
    "    df[colnames] = df[colnames].interpolate(method = 'time', inplace = False, limit = 50, limit_direction = 'both')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing the Time Series for each Variable\n",
    "\n",
    "T, RH and AH are likely non-stationary (seasonal) with yearly seasonality (8760 lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 4\n",
    "ncol = 2\n",
    "x = list(range(len(df)))\n",
    "\n",
    "titles = tuple(('Time vs ' + colnames for colnames in col))\n",
    "\n",
    "fig = make_subplots(rows = nrow, cols = ncol,\n",
    "                   subplot_titles = titles)\n",
    "\n",
    "r = 1\n",
    "c = 1\n",
    "\n",
    "for colnames in col:\n",
    "    fig.append_trace(go.Scatter(\n",
    "    x = df.index, y = df[colnames],\n",
    "    ), row = r, col = c)\n",
    "\n",
    "    fig.update_xaxes(title_text = 'Time',\n",
    "                    row = r, col = c)\n",
    "    fig.update_yaxes(title_text = colnames,\n",
    "                    row = r, col = c)\n",
    "    \n",
    "    c = c+1\n",
    "    if c > ncol:\n",
    "        c = 1\n",
    "        r = r+1\n",
    "    \n",
    "    \n",
    "fig.update_layout(height = 1800, width = 1000, showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rolling(df, column):\n",
    "    df['z_data'] = (df[column] - df[column].rolling(window=12).mean()) / df[column].rolling(window=12).std()\n",
    "    df['zp_data'] = df['z_data'] - df['z_data'].shift(8760)\n",
    "    fig, ax = plt.subplots(3,figsize=(12, 9))\n",
    "    ax[0].plot(df.index, df[column], label='raw data')\n",
    "    ax[0].plot(df[column].rolling(window=12).mean(), label=\"rolling mean\");\n",
    "    ax[0].plot(df[column].rolling(window=12).std(), label=\"rolling std (x10)\");\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(df.index, df.z_data, label=\"de-trended data\")\n",
    "    ax[1].plot(df.z_data.rolling(window=12).mean(), label=\"rolling mean\");\n",
    "    ax[1].plot(df.z_data.rolling(window=12).std(), label=\"rolling std (x10)\");\n",
    "    ax[1].legend()\n",
    "\n",
    "    ax[2].plot(df.index, df.zp_data, label=\"12 lag differenced de-trended data\")\n",
    "    ax[2].plot(df.zp_data.rolling(window=12).mean(), label=\"rolling mean\");\n",
    "    ax[2].plot(df.zp_data.rolling(window=12).std(), label=\"rolling std (x10)\");\n",
    "    ax[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rolling(df_stat, 'precipitation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for Stationarity\n",
    "\n",
    "Use Augmented Dicky-Fuller Test and KPSS, where\n",
    "\n",
    "- p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "- p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n",
    "\n",
    "for Augmented Dicky-Fuller and the reverse for KPSS\n",
    "\n",
    "Errors due to large number (?) so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for colnames in col:\n",
    "    try:\n",
    "        result = adfuller(df[colnames])\n",
    "        result_list.append(\n",
    "            {\n",
    "                'ADF': result[0],\n",
    "                'p-value': result[1],\n",
    "                'lags': result[2], \n",
    "                'num_obs': result[3],\n",
    "                'crit_vals': result[4], \n",
    "                'ic_max': result[5],\n",
    "                'stationary': (result[1] <= 0.05),\n",
    "                'idx': colnames\n",
    "            })\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "df_res = pd.DataFrame(result_list)\n",
    "df_res.index = df_res['idx']\n",
    "df_res = df_res.drop(['idx'], axis = 1)\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for colnames in col:\n",
    "    try:\n",
    "        result = kpss(df[colnames])\n",
    "        result_list.append(\n",
    "            {\n",
    "                'KPSS': result[0],\n",
    "                'p-value': result[1],\n",
    "                'lags': result[2], \n",
    "                'crit_vals': result[3],\n",
    "                'stationary': (result[1] >= 0.05),\n",
    "                'idx': colnames\n",
    "            })\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "df_res = pd.DataFrame(result_list)\n",
    "df_res.index = df_res['idx']\n",
    "df_res = df_res.drop(['idx'], axis = 1)\n",
    "\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-stationarity Cause\n",
    "\n",
    "Some issues with the plot, but the point stands that the cause of non-stationarity for all is only the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# for colnames in col:\n",
    "res = seasonal_decompose(df['precipitation'], model = 'additive')\n",
    "res.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Seasonality\n",
    "\n",
    "All numerical variables are noticably seasonal. DEWP, TEMP and PRES' seasonality are easily seen, while for the rest, there's always a spike on a constant interval (pm2.5 being the least noticable). All of them have yearly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = df\n",
    "non_stat = ['DEWP', 'HUMI', 'PRES', 'TEMP', 'Iws', 'precipitation',\n",
    "       'Iprec']\n",
    "\n",
    "for colnames in non_stat:\n",
    "    df_stat[colnames] = df_stat[colnames] - df_stat[colnames].shift(8760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat['precipitation'] = df_stat['precipitation'] - 2*df_stat['precipitation'].shift(8760) - df_stat['precipitation'].shift(8760*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow = 4\n",
    "ncol = 2\n",
    "x = list(range(len(df_stat)))\n",
    "\n",
    "titles = tuple(('Time vs ' + colnames for colnames in non_stat))\n",
    "\n",
    "fig = make_subplots(rows = nrow, cols = ncol,\n",
    "                   subplot_titles = titles)\n",
    "\n",
    "r = 1\n",
    "c = 1\n",
    "\n",
    "for colnames in non_stat:\n",
    "    fig.append_trace(go.Scatter(\n",
    "    x = x, y = df_stat[colnames],\n",
    "    ), row = r, col = c)\n",
    "\n",
    "    fig.update_xaxes(title_text = 'Time',\n",
    "                    row = r, col = c)\n",
    "    fig.update_yaxes(title_text = colnames,\n",
    "                    row = r, col = c)\n",
    "    \n",
    "    c = c+1\n",
    "    if c > ncol:\n",
    "        c = 1\n",
    "        r = r+1\n",
    "    \n",
    "    \n",
    "fig.update_layout(height = 1500, width = 1000, showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize / Normalize\n",
    "\n",
    "- Normalize if the variable distribution is Gaussian (for reliable result)\n",
    "- Standardize if no visible upward trending, thus max and min should be reliable enough\n",
    "\n",
    "Do we need to do this? Each variable has its own scale...\n",
    "\n",
    "Decide not to Standardize / Normalize (code is still in the cells should it be necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "for colnames in non_stat:\n",
    "    vals = df_stat[colnames].values.reshape(len(df_stat[colnames]), 1)\n",
    "    df_stat[colnames + '_std'] = scaler.fit(vals).transform(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Histogram\n",
    "\n",
    "Checking how the data spread is to help determine whether using parametric or non-parametric test is a better choice. Seems that all of them are Gaussian, and quite explainable by mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nrow = 4\n",
    "ncol = 2\n",
    "\n",
    "titles = tuple(colnames for colnames in non_stat)\n",
    "\n",
    "fig = make_subplots(rows = nrow, cols = ncol,\n",
    "                   subplot_titles = titles)\n",
    "\n",
    "r = 1\n",
    "c = 1\n",
    "\n",
    "for colnames in non_stat:\n",
    "    fig.append_trace(go.Histogram(\n",
    "    x = df_stat[colnames], nbinsx = 160\n",
    "    ), row = r, col = c)\n",
    "\n",
    "    fig.update_xaxes(title_text = colnames,\n",
    "                    row = r, col = c)\n",
    "    \n",
    "    c = c+1\n",
    "    if c > ncol:\n",
    "        c = 1\n",
    "        r = r+1\n",
    "    \n",
    "    \n",
    "fig.update_layout(height = 1500, width = 1000, showlegend = False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Causality Tests\n",
    "\n",
    "https://blog.minitab.com/blog/adventures-in-statistics-2/choosing-between-a-nonparametric-test-and-a-parametric-test\n",
    "\n",
    "Test for causality between the variables...\n",
    "\n",
    "- Separability? Cannot know (no packages implemented in Python yet, high level calculations, not enough time)\n",
    "- Parametric? Yes (data normally distributed)\n",
    "- Do we need a causal graph? At the moment not (though for the later project, as we have tons of sensors this might be needed), so maybe yes?\n",
    "- Is the system observable?\n",
    "    - Is causal sufficiency met? Yes (complete causal graph?)\n",
    "    \n",
    "Test using Granger and PCMCI (both have implementations in Python, and fulfills the requirements above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Granger Causality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "maxlag = 50\n",
    "test = 'ssr_chi2test'\n",
    "\n",
    "def grangers_causation_matrix(data, variables, test = test, maxlag = maxlag, verbose = False):\n",
    "    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns = variables, index = variables)\n",
    "    for c in df.columns:\n",
    "        for r in df.index:\n",
    "            test_result = grangercausalitytests(data[[r,c]], maxlag = maxlag, verbose = False)\n",
    "            p_values = [(test_result[i+1][0][test][1]) for i in range(maxlag)]\n",
    "            \n",
    "            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            df.loc[r,c] = min_p_value\n",
    "    df.columns = [var + '_x' for var in variables]\n",
    "    df.index = [var + '_y' for var in variables]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Grangers Causation\n",
    "\n",
    "This one might be unreliable. A lot of warnings about constraints' covariance not having full rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nonan = df_stat.dropna()\n",
    "grangers_causation_matrix(df_nonan, variables = non_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PCMCI \n",
    "\n",
    "LoOoOoOngGgGGg waiting time like Granger. Most likely because of large dataset that makes the whole thing runs slowly. Results were saved in the results variable while intepretation is printed on the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigramite.pcmci import PCMCI\n",
    "from tigramite.independence_tests import ParCorr\n",
    "import tigramite.plotting as pt\n",
    "import tigramite.data_processing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcmci = df_nonan[non_stat]\n",
    "df_pcmci = pp.DataFrame(df_pcmci.to_numpy())\n",
    "\n",
    "pcmci = PCMCI(df_pcmci, cond_ind_test = ParCorr())\n",
    "results = pcmci.run_pcmci(tau_min = 0, tau_max = 50, pc_alpha = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Variable name according to order.\n",
    "(X Y) means it is connected to Variable X on lag Y\n",
    "'''\n",
    "pcmci.print_significant_links(p_matrix=results['p_matrix'],\n",
    "                            val_matrix=results['val_matrix'],\n",
    "                            alpha_level=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy \n",
    "\n",
    "Check the p-value matrix and the valuation matrix. Change 'val_matrix' and 'p_matrix' for each matrix.\n",
    "The format [X,Y,Z] determines what you want to check. X,Y are the variables (in the same order as the columns),\n",
    "while Z is the lag. Putting \":\" means you check for all inputs possible for that variable.\n",
    "\n",
    "Example: \n",
    "- Checking the 'val_matrix' for variable 0 and variable 1 for all lags\n",
    "\n",
    "    pd.DataFrame(results['val_matrix'][0,1,:])\n",
    "    \n",
    "    \n",
    "- Checking the p-value between all variables in lag 0\n",
    "\n",
    "    pd.DataFrame(results['p_matrix'][:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results['val_matrix'][:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
